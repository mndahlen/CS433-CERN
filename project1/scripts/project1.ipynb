{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "\n",
    "DATA_TRAIN_PATH = '../data/train.csv.zip'\n",
    "DATA_TEST_PATH = '../data/test.csv.zip'\n",
    "\n",
    "# Load data\n",
    "data_train = pd.read_csv(DATA_TRAIN_PATH)\n",
    "data_test = pd.read_csv(DATA_TEST_PATH) # NOTE: This is test for SUBMISSION, not test for training.\n",
    "\n",
    "# Make Y binary [-1,1]\n",
    "data_train.loc[data_train['Prediction'] == 's','Prediction'] = 1\n",
    "data_train.loc[data_train['Prediction'] == 'b','Prediction'] = 0\n",
    "data_train['Prediction'] = pd.to_numeric(data_train['Prediction'])\n",
    "\n",
    "# Extract y\n",
    "y_train = data_train['Prediction'].to_frame()\n",
    "y = y_train.to_numpy()\n",
    "\n",
    "# Get ids\n",
    "id_train = data_train['Id'].to_frame()\n",
    "\n",
    "# Get x\n",
    "x_train = data_train.copy()\n",
    "x_train.drop(['Id','Prediction'], 1, inplace=True) \n",
    "x = x_train.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hackz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom scipy import stats\\nimport statsmodels.api as sm\\nimport statsmodels.formula.api as smf\\n\\nmod = smf.logit(formula='Prediction ~  DER_mass_MMC + DER_mass_transverse_met_lep + DER_mass_vis +                          DER_pt_h + DER_deltaeta_jet_jet + DER_mass_jet_jet +                          DER_pt_ratio_lep_tau + DER_met_phi_centrality + DER_lep_eta_centrality +                          PRI_tau_pt + PRI_tau_eta + PRI_tau_phi + PRI_lep_pt + PRI_lep_eta + PRI_lep_phi +                          PRI_met + PRI_met_phi + PRI_met_sumet + PRI_jet_num + PRI_jet_leading_pt +                          PRI_jet_leading_eta + PRI_jet_leading_phi + PRI_jet_subleading_pt +                          PRI_jet_subleading_eta + PRI_jet_subleading_phi + PRI_jet_all_pt', data=data_train)\\nres = mod.fit()\\nprint(res.summary())\\n\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial survey of data\n",
    "'''\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "mod = smf.logit(formula='Prediction ~  DER_mass_MMC + DER_mass_transverse_met_lep + DER_mass_vis + \\\n",
    "                         DER_pt_h + DER_deltaeta_jet_jet + DER_mass_jet_jet + \\\n",
    "                         DER_pt_ratio_lep_tau + DER_met_phi_centrality + DER_lep_eta_centrality + \\\n",
    "                         PRI_tau_pt + PRI_tau_eta + PRI_tau_phi + PRI_lep_pt + PRI_lep_eta + PRI_lep_phi + \\\n",
    "                         PRI_met + PRI_met_phi + PRI_met_sumet + PRI_jet_num + PRI_jet_leading_pt + \\\n",
    "                         PRI_jet_leading_eta + PRI_jet_leading_phi + PRI_jet_subleading_pt + \\\n",
    "                         PRI_jet_subleading_eta + PRI_jet_subleading_phi + PRI_jet_all_pt', data=data_train)\n",
    "res = mod.fit()\n",
    "print(res.summary())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using above\n",
    "Above can be used to test different models for logistic regression. Test different combinations etc, also get parameters (coefficients). We can begin by only using the predictors with strong coefficients, for example \"DER_pt_ratio_lep_tau\". /Martin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Linear model, logistic regression, standardized, 10% train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0/100000, loss = [[0.69314718]]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-1a393fee581e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Logistical Regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# NOTE: Logistic loss is now normalized with size of valuation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Test model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Skrivbord\\Kurser\\ML\\CS433\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_logistic_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"iter: {}/{}, loss = {}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_logistic_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Skrivbord\\Kurser\\ML\\CS433\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcalculate_logistic_gradient\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_logistic_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;34m\"\"\"compute the gradient of loss.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m     \u001b[0msig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m     \u001b[0mgrad_coefficient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msig\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mgrad_coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "# Normalize data\n",
    "x_mean = np.mean(x)\n",
    "x_std = np.std(x)\n",
    "#print(x_mean)\n",
    "x_norm = (x - x_mean)/x_std\n",
    "\n",
    "# Extend data with more features\n",
    "tx = build_poly_1D(x_norm, 1)\n",
    "\n",
    "# split data\n",
    "x_train,y_train,x_test,y_test = split_data(tx, y, 0.1, seed=1)\n",
    "\n",
    "# initialize a model (Not tested yet)\n",
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "max_iters = 100000\n",
    "gamma = 0.000001\n",
    "\n",
    "# Use cross validation?\n",
    "# Use ridge regression?\n",
    "\n",
    "# Logistical Regression\n",
    "# NOTE: Logistic loss is now normalized with size of valuation data\n",
    "w, loss = logistic_regression(y_train.reshape(y_train.size,1), x_train, initial_w, max_iters, gamma)\n",
    "\n",
    "# Test model\n",
    "te_loss = calculate_logistic_loss(y_test, x_test, w)\n",
    "y_pred = np.rint(sigmoid(x_test@w))\n",
    "diff = y_pred - y_test.reshape(y_test.shape[0],1)\n",
    "print(\"Model classified {} % correct\".format((1 -  sum(abs(diff))/y_test.shape[0])[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 results:\n",
    "iter: 499999/500000, loss = [[0.52571624]]\n",
    "\n",
    "Model classified 0.7393377777777778 % correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Logistic regression SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0/100000, loss = [[0.69314718]]\n",
      "\n",
      "iter: 1000/100000, loss = [[0.6641653]]\n",
      "\n",
      "iter: 2000/100000, loss = [[0.64421798]]\n",
      "\n",
      "iter: 3000/100000, loss = [[0.63254531]]\n",
      "\n",
      "iter: 4000/100000, loss = [[0.62890472]]\n",
      "\n",
      "iter: 5000/100000, loss = [[0.62668246]]\n",
      "\n",
      "iter: 6000/100000, loss = [[0.62661024]]\n",
      "\n",
      "iter: 7000/100000, loss = [[0.62453799]]\n",
      "\n",
      "iter: 8000/100000, loss = [[0.62383786]]\n",
      "\n",
      "iter: 9000/100000, loss = [[0.62271655]]\n",
      "\n",
      "iter: 10000/100000, loss = [[0.62220509]]\n",
      "\n",
      "iter: 11000/100000, loss = [[0.62178964]]\n",
      "\n",
      "iter: 12000/100000, loss = [[0.62119106]]\n",
      "\n",
      "iter: 13000/100000, loss = [[0.61965313]]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-455a816056af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Logistical Regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# NOTE: Logistic loss is now normalized with size of valuation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_SGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Test model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Skrivbord\\Kurser\\ML\\CS433\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mlogistic_regression_SGD\u001b[1;34m(y, tx, initial_w, max_iters, gamma, batch_ratio)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"iter: {}/{}, loss = {}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_logistic_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_mean = np.mean(x)\n",
    "x_std = np.std(x)\n",
    "#print(x_mean)\n",
    "x_norm = (x - x_mean)/x_std\n",
    "\n",
    "# Extend data with more features\n",
    "tx = build_poly_1D(x_norm, 2)\n",
    "\n",
    "# split data\n",
    "x_train,y_train,x_test,y_test = split_data(tx, y, 0.1, seed=1)\n",
    "\n",
    "# initialize a model (Not tested yet)\n",
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "max_iters = 100000\n",
    "#gamma = 0.0002\n",
    "gamma = 0.0000001\n",
    "\n",
    "# Use cross validation?\n",
    "# Use ridge regression?\n",
    "\n",
    "# Logistical Regression\n",
    "# NOTE: Logistic loss is now normalized with size of valuation data\n",
    "w, loss = logistic_regression_SGD(y_train.reshape(y_train.size,1), x_train, initial_w, max_iters, gamma, 0.01)\n",
    "\n",
    "# Test model\n",
    "te_loss = calculate_logistic_loss(y_test, x_test, w)\n",
    "y_pred = np.rint(sigmoid(x_test@w))\n",
    "diff = y_pred - y_test.reshape(y_test.shape[0],1)\n",
    "#print()\n",
    "sys.stdout.write(\"Model classified {} % correct\".format((1 -  sum(abs(diff))/y_test.shape[0])[0]))\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
